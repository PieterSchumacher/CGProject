--- 12/02/2020 through 19/02/2020 ---
Questions for assistant:
  Suggestion for image format, prefer an industry standard  -> .ppm or .png or .jpeg
  Way to read Wavefront Object files with notepad/any other application program -> just unzip the file and open
  Find a way to define a working project structure without h2w lib -> for later, must focus on deadline
  Further explanation on the use of a Pixel data structure, and possible other screen/film related things -> Pixel not necessary

image format: .png of openexr
.gz unzippen en dan notepad
zoek op: stack vs heap

19/02/2020:
  Got the intersections working (or atleast to the point where a 2d image renders). Strange bug occurring
  where the sphere moves downwards if the z coordinate is incremented. Need to rethink the way ray directions, cameras
  and hitpoints are represented (in what coordinate systems?). Also: the image isn't taking the plane offset into
  account yet. The ray direction must change accordingly. Maybe ask help tomorrow. Light calculations are strange,
  must somehow be able to multiply and get a value between 0 and 255

20/02/2020:
  -- Questions --
  Do lines like intersection.object->material->kd add a lot of overhead? How to improve?
  Intersection.cpp line intersection = {smallest_t_so_far, n, object, ray}; what to do with object? if the original object gets deleted, will this too?
  What coordinate systems should the ray direction and/or eye be represented in?
  RGB arithmetic, how to retain relative values within 0 and 255
  Best practices for auxiliary functions like get_intersection_color(), within main.cpp or seperate .h/.cpp?
  -- Notes --
  Made progress on color representation. The sphere now reflects light originating from a point light. Must refine shading
  to support material properties. Basic arithmetic operators like + - / * of the rgb struct passed a reference of the original
  object as a result, which resulted in weird bugs. They now return a new copy. Still need to implement gamma correction,
  the correct calculation for a ray direction (i.e. deriving from the camera axes uvw) and new shapes like triangles etc...

21/02/2020:
  -- Notes --
  Wouldn't it be nicer if the origin of the ray was set to the intersection of the direction with the image plane,
  instead of the eye of the camera? That way no check for t >= min_t is needed in intersection calculations. (27/02/2020: this is definitely not true)

  Rays now 'shoot' correctly through a camera. Did some refactoring relating to intersection methods.
  The coordinate systems still behave strangely, putting eye at (0, 2.5, -10) gives a pink screen, while (1e-6, 2.5, -10)
  does not.

26/02/2020:
  BVH: First hierarchy of scene, then new hierarchy for triangle mesh OR entire hierarchy for scene? Difference in performance?

27/02/2020:
  Visibility & viewing ray intersections are working for all objects. Optimized triangle intersection. Now writing a
  wavefront obj parser. Next up is anti-aliasing or BVH.
  -- Questions --
  Seemingly left-handedness for eigen vector operations?
  Easy way to parallelize the outer loop?
  Clamping & color representation still not 100% clear
  Difference in complexity between nearest intersection for viewing ray and computing reflected light radiances
  -- Notes --
  Debugged the wavefront parser. Shading normals are now interpolated according to Phong interpolation.
  Anti-aliasing makes seemingly no difference. Can test this by plotting the color values before and after to see
  if the curve smooths because of super sampling.
  BVH representation -> Composite pattern (some papers say this is slow because of virtual function calls)
  2D/3D transformations -> Visitor pattern

28/02/2020:
  On the use of design patterns:
  BVH construction might be best done with a Builder pattern, however should look into other creational patterns to
  assess whether this gives the most utility overall. Why? Because BVH construction can be done in several ways,
  and it might be useful to make the code reusable and adaptable to several different construction algorithms.
  Also, transformations must also work for any type of light source. Thinking about whether or not it would be
  prudent to make a more general abstract class which Object and Light inherit from, and let transformations call
  on that.
  --> after reading some papers, some of these things are actually used. Object and Light usually inherit from a more
      abstract class Primitive. Acceleration structures often have Builders. Composite pattern for BVH representation is
      often claimed to be inefficient because of the way virtual function calls work. Haven't found any data to back up
      these claims however. Function templates might solve this. Does this mean for loops over polymorphic vectors are
      inefficient as well?

04/03/2020:
  Wrote a draft for BVH definition, intersection and construction. Recursive definition
  is giving problems, however the intersection code is very compact. Construction is done using
  a builder and a recursive method. This has a strange problem where defining BVH bvh at the start does not
  work because you need to construct either a leaf or a node, which have two different constructors.
  Something strange is going on with the intersection method in BVH and the constructor for BVH.
  Also, sometimes the initializer syntax doesn't work (for example when initializing Vector3ds in Sphere.
  Starting with draft for AreaLight today, tomorrow try to fix the BVH bugs with help from assistant.